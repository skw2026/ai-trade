# 数据加速链路（归档 + 增量 + 回补 + 特征 + 回测）

## 1. 目标

在不破坏现有 `train/assess/full` 闭环的前提下，新增一条可调度的数据加速链路：

1. 归档下载（Binance archive）。
2. 实时增量（Bybit 最新 K 线增量同步）。
3. 缺口回补（按周期检测并补齐缺失时间戳）。
4. 特征构建（生成 feature store）。
5. walk-forward 回测（快速验证参数与信号有效性）。

入口统一为：
- `tools/closed_loop_runner.sh data`
- 内部调用 `tools/run_data_pipeline.py --config config/data_pipeline.yaml`

## 2. 配置文件

默认配置：`config/data_pipeline.yaml`

核心字段：

- `common.symbol / common.interval_minutes / common.category`
- `paths.ohlcv_csv / paths.feature_csv / paths.backtest_report`
- `archive.*`：归档下载参数
- `incremental.*`：增量同步参数
- `gap_fill.*`：缺口回补参数
- `feature_store.*`：特征构建参数
- `walkforward.*`：回测参数

## 3. 本地执行

```bash
# 仅跑数据加速链路
tools/closed_loop_runner.sh data \
  --compose-file docker-compose.prod.yml \
  --env-file .env.runtime \
  --data-config config/data_pipeline.yaml
```

也可单独执行：

```bash
python3 tools/run_data_pipeline.py \
  --config config/data_pipeline.yaml \
  --run-dir data/reports/closed_loop/manual_data_pipeline
```

## 4. 调度执行（scheduler）

将 `SCHEDULER_ACTION` 设为 `data` 即可周期执行该链路：

```bash
SCHEDULER_ACTION=data
SCHEDULER_INTERVAL_SECONDS=21600
DATA_PIPELINE_CONFIG=config/data_pipeline.yaml
```

注意：
- 默认 `SCHEDULER_ACTION=full` 不变，保证现有生产闭环行为不受影响。
- 仅在需要加速数据学习/回测时切换到 `data`，或新增独立计划任务容器。

## 5. 产物路径

每次执行会在闭环 run 目录下写入：

- `data_pipeline/archive_report.json`
- `data_pipeline/incremental_report.json`
- `data_pipeline/gap_fill_report.json`
- `data_pipeline/feature_store_report.json`
- `data_pipeline/data_pipeline_report.json`
- `walkforward_report.json`
- `feature_store_5m.csv`

若 action= `data`，仍会生成 `run_meta.json` 和 `closed_loop_report.json`，但不会刷新 `latest_runtime_assess.json` 指针（与现有 `train` 行为一致）。
